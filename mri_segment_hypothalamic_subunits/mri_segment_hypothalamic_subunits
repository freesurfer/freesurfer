#!/usr/bin/env python

import os
import sys
import csv
import glob
import logging
import numpy as np
import nibabel as nib
import freesurfer as fs
from copy import deepcopy
from scipy.ndimage import label

# make sure tensorflow is installed in fspython
fs.utils.check_tensorflow()

import tensorflow as tf
from tensorflow import keras

# set tensorflow logging
tf.get_logger().setLevel(logging.ERROR)
logging.getLogger('tensorflow').disabled = True


# ================================================================================================
#                                         Main Entrypoint
# ================================================================================================


def main():

    # configure commandline
    parser = fs.utils.ArgumentParser(description="This module can be run in two modes: a) on FreeSurfer subjects, and "
                                                 "b) on any T1-weighted scan(s) of approximatively 1mm resolution. ",
                                     epilog='\n\n')

    # FreeSurfer mode
    parser.add_argument("--s", nargs='*',
                        help="(required in FS mode) Name of one or several subjects in $SUBJECTS_DIR on which to run "
                             "mri_segment_hypothalamic_subunits, "
                             "assuming recon-all has been run on the specified subjects. "
                             "The output segmentations will automatically be saved in each subject's mri folder. "
                             "If no argument is given, mri_segment_hypothalamic_subunits will run on all the subjects "
                             "in $SUBJECTS_DIR.")
    parser.add_argument("--sd", help="(FS mode, optional) override current $SUBJECTS_DIR")
    parser.add_argument("--write_posteriors", action="store_true", help="(FS mode, optional) save posteriors, "
                                                                        "default is False")

    # normal mode
    parser.add_argument("--i", help="(required in T1 mode) Image(s) to segment. "
                                    "Can be a path to a single image or to a folder.")
    parser.add_argument("--o", help="(required in T1 mode) Segmentation output. "
                                    "Must be the same type as input path (single file or folder).")
    parser.add_argument("--out_posteriors", help="(T1 mode, optional) Posteriors output. "
                                                 "Must be the same type as input path (single file or folder).")
    parser.add_argument("--out_volumes", help="(T1 mode, optional) Path of a csv file where the volumes of the "
                                              "hypothalamic subunits will be saved.")

    # in both cases
    parser.add_argument("--model", help=fs.utils.parser.argparse.SUPPRESS)
    parser.add_argument("--threads", type=int, default=1, help="(both modes, optional) Number of cores to be used. "
                                                               "Default uses 1 core.")

    # check for no arguments
    if len(sys.argv) < 2:
        parser.print_help()
        sys.exit(1)

    # parse commandline
    args = parser.parse_args()

    # locate model weights
    if args.model:
        model = args.model
    elif not fs.fshome():
        model = None
        fs.fatal('FREESURFER_HOME is not set. Please source freesurfer.')
    else:
        model = os.path.join(fs.fshome(), 'average', 'hypothalamic_subunits_model.h5')

    # run prediction
    predict(
        name_subjects=args.s,
        path_subjects_dir=args.sd,
        write_posteriors_FS=args.write_posteriors,
        path_images=args.i,
        path_segmentations=args.o,
        path_model=model,
        path_posteriors=args.out_posteriors,
        path_volumes=args.out_volumes,
        threads=args.threads
    )


# ================================================================================================
#                                 Prediction and Processing Utilities
# ================================================================================================


def predict(name_subjects=None,
            path_subjects_dir=None,
            write_posteriors_FS=False,
            path_images=None,
            path_segmentations=None,
            path_model='../data/model.h5',
            path_posteriors=None,
            path_volumes=None,
            threads=1):
    '''
    Prediction pipeline.
    '''

    assert path_model, "A model file is necessary"
    print('')

    # prepare output filepaths
    images_to_segment, path_segmentations, path_posteriors, path_volumes, path_main_volumes, path_stats = \
        prepare_output_files(
            name_subjects,
            path_subjects_dir,
            write_posteriors_FS,
            path_images,
            path_segmentations,
            path_posteriors,
            path_volumes)

    # get label and classes lists
    label_list = np.concatenate([np.zeros(1, dtype='int32'), np.arange(801, 811)])

    # prepare volume files if needed
    if path_main_volumes is not None:
        write_csv_file(volumes=None, filename=path_main_volumes, subject=None, write_header=True, open_type='w')

    if threads == 1:
        print('Using 1 thread\n')
    else:
        print('Using %s threads\n' % threads)
    sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=threads, inter_op_parallelism_threads=threads))
    with sess:

        # perform segmentation
        net = None
        previous_model_input_shape = None
        for idx, (path_image, path_segmentation, path_posterior, path_volume, path_stat) in \
                enumerate(zip(images_to_segment, path_segmentations, path_posteriors, path_volumes, path_stats)):
            print_loop_info(idx, len(images_to_segment), 10)

            # preprocess image and get information
            try:
                image, aff, h, im_res, n_channels, n_dims, shape, cropping, crop_idx = preprocess_image(path_image)
                model_input_shape = list(image.shape[1:])
            except Exception as e:
                print('\nthe following problem occured when preprocessing image %s :' % path_image)
                print(e)
                print('resuming program execution\n')
                continue

            # prepare net for first image or if input's size has changed
            if (idx == 0) | (previous_model_input_shape != model_input_shape):

                # check for image size compatibility
                if (idx != 0) & (previous_model_input_shape != model_input_shape):
                    print('image of different shape as previous ones, redefining network')
                previous_model_input_shape = model_input_shape
                net = build_model(path_model, model_input_shape, len(label_list))

            # predict posteriors
            try:
                prediction_patch = net.predict(image)
            except Exception as e:
                print('\nthe following problem occured when predicting segmentation of image %s :' % path_image)
                print(e)
                print('\nresuming program execution')
                continue

            # get posteriors and segmentation
            try:
                seg, posteriors = postprocess(prediction_patch, cropping, shape, crop_idx, n_dims, label_list, aff)
            except Exception as e:
                print('\nthe following problem occured when postprocessing segmentation %s :' % path_segmentation)
                print(e)
                print('\nresuming program execution')
                continue

            # compute volumes
            try:
                if (path_main_volumes is not None) | (path_volume is not None):  # compute volumes only if necessary
                    volumes = np.sum(posteriors[..., 1:], axis=tuple(range(0, len(posteriors.shape) - 1)))
                    volumes = volumes * np.prod(im_res)
                    volumes_whole = np.array([np.sum(volumes[:int(len(volumes) / 2)]),
                                              np.sum(volumes[int(len(volumes) / 2):])])
                    volumes = np.concatenate([volumes, volumes_whole])
                    if name_subjects is None:  # any T1 mode
                        subject_name = os.path.basename(path_image).replace('.nii.gz', '')
                    else:  # FS mode
                        subject_name = os.path.basename(os.path.dirname(os.path.dirname(path_image)))
                    if path_main_volumes is not None: # append volumes to main file (regrouping volumes of all subjects)
                        write_csv_file(volumes, path_main_volumes, subject_name, write_header=False, open_type='a')
                    if path_volume is not None:  # create individual volume file in each subject subdirectory (FS mode)
                        write_csv_file(volumes, path_volume, subject_name, write_header=True, open_type='w')
                    if path_stat is not None:  # create individual stats file in each subject subdirectory (FS mode)
                        write_fs_stats_file(volumes, path_stat)
            except Exception as e:
                print('\nthe following problem occured when computing the volumes of '
                      'segmentation %s :' % path_segmentation)
                print(e)
                print('\nresuming program execution')
                continue

            # write results to disk
            try:
                if path_segmentation is not None:
                    save_volume(seg.astype('int'), aff, h, path_segmentation)
                if path_posterior is not None:
                    if n_channels > 1:
                        new_shape = list(posteriors.shape)
                        new_shape.insert(-1, 1)
                        new_shape = tuple(new_shape)
                        posteriors = np.reshape(posteriors, new_shape)
                    save_volume(posteriors.astype('float'), aff, h, path_posterior)
            except Exception as e:
                print('\nthe following problem occured when saving the results for image %s :' % path_image)
                print(e)
                print('\nresuming program execution')
                continue

    # print output info
    if len(path_segmentations) == 1:  # either one image or one subject
        print('\n\nsegmentation saved in: ' + path_segmentations[0])
        if path_posteriors[0] is not None:
            print('posteriors saved in:   ' + path_posteriors[0])
        if path_volumes[0] is not None:  # for FS subject
            print('volumes saved in:      ' + path_volumes[0])
        if path_main_volumes is not None:  # for single image
            print('volumes saved in:      ' + path_main_volumes)
    else:
        if name_subjects is None:  # images in folder
            print('\n\nsegmentations saved in: ' + os.path.dirname(path_segmentations[0]))
            if path_posteriors[0] is not None:
                print('posteriors saved in:    ' + os.path.dirname(path_posteriors[0]))
            if path_main_volumes is not None:
                print('volumes saved in:       ' + path_main_volumes)
        else:  # several subjects
            if path_posteriors[0] is not None:
                print('\n\nsegmentations, posteriors, and individual subject volumes saved in each subject directory')
            else:
                print('\n\nsegmentations and individual subject volumes saved in each subject directory')
            print('additional file regrouping the volumes of all subjects saved in: ' + path_main_volumes)

    print('\n\nIf you use this tool in a publication, please cite:')
    print('Automated segmentation of the hypothalamus and associated subunits in brain MRI')
    print('B. Billot, M. Bocchetta, E. Todd, A. V. Dalca, J. D. Rohrer, J. E. Iglesias')
    print('NeuroImage (in press)\n\n')


def prepare_output_files(name_subjects, subjects_dir, write_posteriors_FS, path_images, out_seg, out_posteriors,
                         main_volumes):
    '''
    Prepare output files.
    '''

    # variable that tracks if warnings are printed when initialising the output paths
    printed_warning = False

    # goes through all the possibilities of specifying inputs
    if path_images is not None:

        # check other inputs
        assert out_seg is not None, 'please specify an output file/folder (--o) when using flag --i'
        assert name_subjects is None, 'please choose between flags --i and --s, they cannot be used at the same time'
        if subjects_dir is not None:
            print('WARNING: $SUBJECT_DIR not used when flags --i and --o are specified, ignoring value of flag --sd')
            printed_warning = True
        if write_posteriors_FS:
            print('WARNING: flag --write_posteriors not used whith flag --i, ignoring flag --write_posteriors.')
            print('WARNING: If you wish to write the posteriors in the T1 mode, '
                  'please use --out_posteriors <path_posteriors>.')
            printed_warning = True

        # convert path to absolute paths
        path_images = os.path.abspath(path_images)
        if out_seg is not None:
            out_seg = os.path.abspath(out_seg)
        if out_posteriors is not None:
            out_posteriors = os.path.abspath(out_posteriors)
        if main_volumes is not None:
            main_volumes = os.path.abspath(main_volumes)

        # prepare input/output volumes
        if ('.nii.gz' not in path_images) & ('.nii' not in path_images) & ('.mgz' not in path_images) & \
                ('.npz' not in path_images):
            images_to_segment = list_images_in_folder(path_images)
            assert len(images_to_segment) > 0, "Could not find any training data"
            if out_seg:
                if not os.path.exists(out_seg):
                    os.mkdir(out_seg)
                out_seg = [os.path.join(out_seg, os.path.basename(image)).replace('.nii', '_seg.nii') for image in
                           images_to_segment]
                out_seg = [seg_path.replace('.mgz', '_seg.mgz') for seg_path in out_seg]
                out_seg = [seg_path.replace('.npz', '_seg.npz') for seg_path in out_seg]
            else:
                out_seg = [out_seg] * len(images_to_segment)
            if out_posteriors:
                if not os.path.exists(out_posteriors):
                    os.mkdir(out_posteriors)
                out_posteriors = [os.path.join(out_posteriors, os.path.basename(image)).replace('.nii',
                                  '_posteriors.nii') for image in images_to_segment]
                out_posteriors = [posteriors_path.replace('.mgz', '_posteriors.mgz')
                                  for posteriors_path in out_posteriors]
                out_posteriors = [posteriors_path.replace('.npz', '_posteriors.npz')
                                  for posteriors_path in out_posteriors]
            else:
                out_posteriors = [out_posteriors] * len(images_to_segment)

        else:
            assert os.path.exists(path_images), "Could not find image to segment"
            images_to_segment = [path_images]
            if out_seg is not None:
                if ('.nii.gz' not in out_seg) & ('.nii' not in out_seg) & ('.mgz' not in out_seg) & \
                        ('.npz' not in out_seg):
                    if not os.path.exists(out_seg):
                        os.mkdir(out_seg)
                    filename = os.path.basename(path_images).replace('.nii', '_seg.nii')
                    filename = filename.replace('mgz', '_seg.mgz')
                    filename = filename.replace('.npz', '_seg.npz')
                    out_seg = os.path.join(out_seg, filename)
                else:
                    if not os.path.exists(os.path.dirname(out_seg)):
                        os.mkdir(os.path.dirname(out_seg))
            out_seg = [out_seg]
            if out_posteriors is not None:
                if ('.nii.gz' not in out_posteriors) & ('.nii' not in out_posteriors) & \
                        ('.mgz' not in out_posteriors) & ('.npz' not in out_posteriors):
                    if not os.path.exists(out_posteriors):
                        os.mkdir(out_posteriors)
                    filename = os.path.basename(path_images).replace('.nii', '_posteriors.nii')
                    filename = filename.replace('mgz', '_posteriors.mgz')
                    filename = filename.replace('.npz', '_posteriors.npz')
                    out_posteriors = os.path.join(out_posteriors, filename)
                else:
                    if not os.path.exists(os.path.dirname(out_posteriors)):
                        os.mkdir(os.path.dirname(out_posteriors))
            out_posteriors = [out_posteriors]

        if main_volumes is not None:
            if main_volumes[-4:] != '.csv':
                print('out_volumes provided without csv extension. Adding csv extension to output_volumes.')
                printed_warning = True
                main_volumes += '.csv'
            if not os.path.exists(os.path.dirname(main_volumes)):
                os.mkdir(os.path.dirname(main_volumes))
        out_volumes = [None] * len(images_to_segment)
        out_stats = [None] * len(images_to_segment)

    # FS mode, run on either a subset of subjects, or on all subjects, with an option to override $SUBJECTS_DIR
    elif name_subjects is not None:

        # check whether addional flags have been provided
        if out_seg is not None:
            print('WARNING: in FS mode segmentations are automatically saved in each subject directory, '
                  'ignoring value provided in --o')
            printed_warning = True
        if main_volumes is not None:
            print('WARNING: in FS mode volumes are automatically saved in each subject directory, '
                  'ignoring value provided in --out_volumes')
            printed_warning = True
        if write_posteriors_FS:
            if out_posteriors is not None:
                print('WARNING: posteriors will automatically be saved in specified subject directory, '
                      'ignoring value provided in --out_posteriors')
                printed_warning = True
            else:
                print("Posteriors will be saved in each subject's directory.")
                printed_warning = True
        elif out_posteriors is not None:  # write_posteriors is False and out_posteriros was specified
            print('WARNING: flag --out_posteriors not used in FS mode, ignoring flag --out_posteriors.')
            print('WARNING: If you wish to write the posteriors in the FS mode, '
                  'please append --write_posteriors to your command line.')
            printed_warning = True

        # override SUBJECTS_DIR if necessary
        if subjects_dir is not None:
            subjects_dir = os.path.abspath(subjects_dir)
            assert os.path.isdir(subjects_dir), "Could not find " + subjects_dir
            os.environ['SUBJECTS_DIR'] = subjects_dir
        else:
            subjects_dir = os.environ['SUBJECTS_DIR']

        # list subjects in SUBJECTS_DIR if none were given
        if not name_subjects:
            name_subjects = list_subfolders(subjects_dir, whole_path=False)

        images_to_segment = list()
        out_seg = list()
        out_posteriors = list()
        out_volumes = list()
        out_stats = list()
        for name in name_subjects:

            # check that provided subject dir
            subject_dir = os.path.join(subjects_dir, name)
            assert os.path.isdir(subject_dir), "Could not find subject dir " + subject_dir
            image_to_segment = os.path.join(subject_dir, 'mri', 'nu.mgz')

            # build paths if input image exists
            if os.path.isfile(image_to_segment):
                images_to_segment.append(image_to_segment)
                out_seg.append(os.path.join(subject_dir, 'mri', 'hypothalamic_subunits_seg.v1.mgz'))
                out_volumes.append(os.path.join(subject_dir, 'mri', 'hypothalamic_subunits_volumes.v1.csv'))
                out_stats.append(os.path.join(subject_dir, 'stats', 'hypothalamic_subunits_volumes.v1.stats'))
                if write_posteriors_FS:
                    out_posteriors.append(os.path.join(subject_dir, 'mri', 'hypothalamic_subunits_posteriors.v1.mgz'))
                else:
                    out_posteriors.append(None)
            else:
                print('WARNING: no such file: ' + image_to_segment + ', continuing program execution without this file')
                printed_warning = True

        # check that we have a least one valid image
        assert len(images_to_segment) > 0, "Could not find any image to segment"

        # if several subjects are run, all volumes are regrouped in a single file
        if len(images_to_segment) > 1:
            main_volumes = os.path.join(subjects_dir, 'hypothalamic_subunits_volumes_all.v1.csv')
        else:
            main_volumes = None

    else:
        fs.fatal('please provide an input image/directory (--i), or a specific FreeSurfer subject directory (--s), '
                 'or the FreeSurfer $SUBJECT_DIR (--sd)')
        images_to_segment = None
        out_volumes = None
        out_stats = None

    if printed_warning:
        print('')

    return images_to_segment, out_seg, out_posteriors, out_volumes, main_volumes, out_stats


def write_csv_file(volumes, filename, subject, write_header, open_type):
    '''
    Writes volumetric stats to csv file.
    :param volumes: numpy array with volumes for all structures specified in below header
    :param filename: path of csv file where volumes will be saved
    :param subject: subject name
    :param write_header: whether to write the header or not
    :param open_type: can be 'w' (write new file), or 'a' (add volume to an already existing file)
    '''
    header = [
        'subject',
        'left anterior-inferior',
        'left anterior-superior',
        'left posterior',
        'left tubular inferior',
        'left tubular superior',
        'right anterior-inferior',
        'right anterior-superior',
        'right posterior',
        'right tubular inferior',
        'right tubular superior',
        'whole left',
        'whole right'
    ]
    with open(filename, open_type) as csv_file:
        writer = csv.writer(csv_file)
        if write_header:
            writer.writerow(header)
        if volumes is not None:
            datarow = [subject] + ['%.3f' % vol for vol in volumes]
            writer.writerow(datarow)
    csv_file.close()


def write_fs_stats_file(volumes, filename):
    '''
    Write volumetric stats to FS stats file.
    '''
    segnames = [
        'Left-Anterior-Inferior',
        'Left-Anterior-Superior',
        'Left-Posterior',
        'Left-Tubular-Inferior',
        'Left-Tubular-Superior',
        'Right-Anterior-Inferior',
        'Right-Anterior-Ssuperior',
        'Right-Posterior',
        'Right-Tubular-Inferior',
        'Right-Tubular-Superior',
        'Whole-Left',
        'Whole-Right'
    ]
    volumes = ['%.3f' % vol for vol in volumes]
    volwidth = len(max(volumes, key=len))
    with open(filename, 'w') as f:
        f.write('# Hypothalamic Subunit Volumetric Stats\n')
        for i, vol in enumerate(volumes):
            f.write('%s  %s  0  %s  %s\n' % (
                str(i + 1).rjust(2),
                str(i + 1).rjust(2),
                vol.rjust(volwidth),
                segnames[i],
            ))


def preprocess_image(im_path):
    '''
    Input image pre-processing.
    '''

    # read image and corresponding info
    n_levels = 3
    im, shape, aff, n_dims, n_channels, header, im_res = get_volume_info(im_path,
                                                                               aff_ref=np.eye(4),
                                                                               return_volume=True)

    # check that patch_shape or im_shape are divisible by 2**n_levels
    if not all([size % (2**n_levels) == 0 for size in shape]):
        crop_shape = [min(find_closest_number_divisible_by_m(size, 2 ** n_levels), 232) for size in shape]
    else:
        if not all([size <= 232 for size in shape]):
            crop_shape = [min(size, 232) for size in shape]
        else:
            crop_shape = None

    # crop image if necessary
    if crop_shape is not None:
        crop_idx = np.round((shape - np.array(crop_shape)) / 2).astype('int')
        crop_idx = np.concatenate((crop_idx, crop_idx + crop_shape), axis=0)
        im = crop_volume_with_idx(im, crop_idx=crop_idx)
    else:
        crop_idx = None

    # normalise image
    m = np.min(im)
    M = np.max(im)
    if M == m:
        im = np.zeros(im.shape)
    else:
        im = (im - m) / (M - m)

    # add batch and channel axes
    if n_channels > 1:
        im = add_axis(im)
    else:
        im = add_axis(im, -2)

    return im, aff, header, im_res, n_channels, n_dims, shape, crop_shape, crop_idx


def build_model(model_file, input_shape, n_lab):
    '''
    Builds keras unet model.
    '''

    # build UNet
    batch_norm_dim = -1
    net = unet(nb_features=24,
                          input_shape=input_shape,
                          nb_levels=3,
                          conv_size=3,
                          nb_labels=n_lab,
                          name='unet',
                          prefix=None,
                          feat_mult=2,
                          pool_size=2,
                          padding='same',
                          dilation_rate_mult=1,
                          activation='elu',
                          use_residuals=False,
                          final_pred_activation='softmax',
                          nb_conv_per_level=2,
                          layer_nb_feats=None,
                          conv_dropout=0,
                          batch_norm=batch_norm_dim,
                          input_model=None)
    net.load_weights(model_file, by_name=True)

    return net


def postprocess(prediction, crop_shape, im_shape, crop, n_dims, labels, aff):
    '''
    Prediction post-processing.
    '''

    # get posteriors and segmentation
    post_patch = np.squeeze(prediction)
    seg_patch = post_patch.argmax(-1)

    # keep biggest connected component (use it with smoothing!)
    seg_left = deepcopy(seg_patch)
    seg_left[seg_left > 5] = 0
    components, n_components = label(seg_left, np.ones([n_dims]*n_dims))
    if n_components > 1:
        unique_components = np.unique(components)
        size = 0
        mask = None
        for comp in unique_components[1:]:
            tmp_mask = components == comp
            tmp_size = np.sum(tmp_mask)
            if tmp_size > size:
                size = tmp_size
                mask = tmp_mask
        seg_left[np.logical_not(mask)] = 0

    seg_right = deepcopy(seg_patch)
    seg_right[seg_right < 6] = 0
    components, n_components = label(seg_right, np.ones([n_dims]*n_dims))
    if n_components > 1:
        unique_components = np.unique(components)
        size = 0
        mask = None
        for comp in unique_components[1:]:
            tmp_mask = components == comp
            tmp_size = np.sum(tmp_mask)
            if tmp_size > size:
                size = tmp_size
                mask = tmp_mask
        seg_right[np.logical_not(mask)] = 0

    seg_patch = seg_left | seg_right

    # paste patches back to matrix of original image size
    if crop_shape is not None:
        seg = np.zeros(shape=im_shape, dtype='int32')
        posteriors = np.zeros(shape=[*im_shape, labels.shape[0]])
        posteriors[..., 0] = np.ones(im_shape)  # place background around patch
        if n_dims == 2:
            seg[crop[0]:crop[2], crop[1]:crop[3]] = seg_patch
            posteriors[crop[0]:crop[2], crop[1]:crop[3], :] = post_patch
        elif n_dims == 3:
            seg[crop[0]:crop[3], crop[1]:crop[4], crop[2]:crop[5]] = seg_patch
            posteriors[crop[0]:crop[3], crop[1]:crop[4], crop[2]:crop[5], :] = post_patch
    else:
        seg = seg_patch
        posteriors = post_patch
    seg = labels[seg.astype('int')].astype('int')

    # align prediction back to first orientation
    seg = align_volume_to_ref(seg, np.eye(4), aff_ref=aff)
    posteriors = align_volume_to_ref(posteriors, np.eye(4), aff_ref=aff, n_dims=n_dims)

    return seg, posteriors


# ================================================================================================
#                       Neurite Utilities - See github.com/adalca/neurite
# 
# TODO: We should import neurite as an external library, but since it's not on pypi yet, let's
# just copy in the needed functions below for now.
# ================================================================================================


def unet(nb_features,
         input_shape,
         nb_levels,
         conv_size,
         nb_labels,
         name='unet',
         prefix=None,
         feat_mult=1,
         pool_size=2,
         padding='same',
         dilation_rate_mult=1,
         activation='elu',
         use_residuals=False,
         final_pred_activation='softmax',
         nb_conv_per_level=1,
         layer_nb_feats=None,
         conv_dropout=0,
         batch_norm=None,
         input_model=None):
    """
    Unet-style keras model with an overdose of parametrization. Copied with permission
    from github.com/adalca/neurite.
    """

    # naming
    model_name = name
    if prefix is None:
        prefix = model_name

    # volume size data
    ndims = len(input_shape) - 1
    if isinstance(pool_size, int):
        pool_size = (pool_size,) * ndims

    # get encoding model
    enc_model = conv_enc(nb_features,
                         input_shape,
                         nb_levels,
                         conv_size,
                         name=model_name,
                         prefix=prefix,
                         feat_mult=feat_mult,
                         pool_size=pool_size,
                         padding=padding,
                         dilation_rate_mult=dilation_rate_mult,
                         activation=activation,
                         use_residuals=use_residuals,
                         nb_conv_per_level=nb_conv_per_level,
                         layer_nb_feats=layer_nb_feats,
                         conv_dropout=conv_dropout,
                         batch_norm=batch_norm,
                         input_model=input_model)

    # get decoder
    # use_skip_connections=True makes it a u-net
    lnf = layer_nb_feats[(nb_levels * nb_conv_per_level):] if layer_nb_feats is not None else None
    dec_model = conv_dec(nb_features,
                         None,
                         nb_levels,
                         conv_size,
                         nb_labels,
                         name=model_name,
                         prefix=prefix,
                         feat_mult=feat_mult,
                         pool_size=pool_size,
                         use_skip_connections=True,
                         padding=padding,
                         dilation_rate_mult=dilation_rate_mult,
                         activation=activation,
                         use_residuals=use_residuals,
                         final_pred_activation=final_pred_activation,
                         nb_conv_per_level=nb_conv_per_level,
                         batch_norm=batch_norm,
                         layer_nb_feats=lnf,
                         conv_dropout=conv_dropout,
                         input_model=enc_model)
    final_model = dec_model

    return final_model


def conv_enc(nb_features,
             input_shape,
             nb_levels,
             conv_size,
             name=None,
             prefix=None,
             feat_mult=1,
             pool_size=2,
             dilation_rate_mult=1,
             padding='same',
             activation='elu',
             layer_nb_feats=None,
             use_residuals=False,
             nb_conv_per_level=2,
             conv_dropout=0,
             batch_norm=None,
             input_model=None):
    """
    Fully Convolutional Encoder. Copied with permission from github.com/adalca/neurite.
    """

    # naming
    model_name = name
    if prefix is None:
        prefix = model_name

    # first layer: input
    name = '%s_input' % prefix
    if input_model is None:
        input_tensor = keras.layers.Input(shape=input_shape, name=name)
        last_tensor = input_tensor
    else:
        input_tensor = input_model.inputs
        last_tensor = input_model.outputs
        if isinstance(last_tensor, list):
            last_tensor = last_tensor[0]
        last_tensor = keras.layers.Reshape(input_shape)(last_tensor)
        input_shape = last_tensor.shape.as_list()[1:]

    # volume size data
    ndims = len(input_shape) - 1
    input_shape = tuple(input_shape)
    if isinstance(pool_size, int):
        pool_size = (pool_size,) * ndims

    # prepare layers
    convL = getattr(keras.layers, 'Conv%dD' % ndims)
    conv_kwargs = {'padding': padding, 'activation': activation, 'data_format': 'channels_last'}
    maxpool = getattr(keras.layers, 'MaxPooling%dD' % ndims)

    # down arm:
    # add nb_levels of conv + ReLu + conv + ReLu. Pool after each of first nb_levels - 1 layers
    lfidx = 0  # level feature index
    for level in range(nb_levels):
        lvl_first_tensor = last_tensor
        nb_lvl_feats = np.round(nb_features * feat_mult ** level).astype(int)
        conv_kwargs['dilation_rate'] = dilation_rate_mult ** level

        for conv in range(nb_conv_per_level):  # does several conv per level, max pooling applied at the end
            if layer_nb_feats is not None:  # None or List of all the feature numbers
                nb_lvl_feats = layer_nb_feats[lfidx]
                lfidx += 1

            name = '%s_conv_downarm_%d_%d' % (prefix, level, conv)
            if conv < (nb_conv_per_level - 1) or (not use_residuals):
                last_tensor = convL(nb_lvl_feats, conv_size, **conv_kwargs, name=name)(last_tensor)
            else:  # no activation
                last_tensor = convL(nb_lvl_feats, conv_size, padding=padding, name=name)(last_tensor)

            if conv_dropout > 0:
                # conv dropout along feature space only
                name = '%s_dropout_downarm_%d_%d' % (prefix, level, conv)
                noise_shape = [None, *[1] * ndims, nb_lvl_feats]
                last_tensor = keras.layers.Dropout(conv_dropout, noise_shape=noise_shape)(last_tensor)

        if use_residuals:
            convarm_layer = last_tensor

            # the "add" layer is the original input
            # However, it may not have the right number of features to be added
            nb_feats_in = lvl_first_tensor.get_shape()[-1]
            nb_feats_out = convarm_layer.get_shape()[-1]
            add_layer = lvl_first_tensor
            if nb_feats_in > 1 and nb_feats_out > 1 and (nb_feats_in != nb_feats_out):
                name = '%s_expand_down_merge_%d' % (prefix, level)
                last_tensor = convL(nb_lvl_feats, conv_size, **conv_kwargs, name=name)(lvl_first_tensor)
                add_layer = last_tensor

                if conv_dropout > 0:
                    name = '%s_dropout_down_merge_%d_%d' % (prefix, level, conv)
                    noise_shape = [None, *[1] * ndims, nb_lvl_feats]
                    last_tensor = keras.layers.Dropout(conv_dropout, noise_shape=noise_shape)(last_tensor)

            name = '%s_res_down_merge_%d' % (prefix, level)
            last_tensor = keras.layers.add([add_layer, convarm_layer], name=name)

            name = '%s_res_down_merge_act_%d' % (prefix, level)
            last_tensor = keras.layers.Activation(activation, name=name)(last_tensor)

        if batch_norm is not None:
            name = '%s_bn_down_%d' % (prefix, level)
            last_tensor = keras.layers.BatchNormalization(axis=batch_norm, name=name)(last_tensor)

        # max pool if we're not at the last level
        if level < (nb_levels - 1):
            name = '%s_maxpool_%d' % (prefix, level)
            last_tensor = maxpool(pool_size=pool_size, name=name, padding=padding)(last_tensor)

    # create the model and return
    model = keras.Model(inputs=input_tensor, outputs=[last_tensor], name=model_name)
    return model


def conv_dec(nb_features,
             input_shape,
             nb_levels,
             conv_size,
             nb_labels,
             name=None,
             prefix=None,
             feat_mult=1,
             pool_size=2,
             use_skip_connections=False,
             padding='same',
             dilation_rate_mult=1,
             activation='elu',
             use_residuals=False,
             final_pred_activation='softmax',
             nb_conv_per_level=2,
             layer_nb_feats=None,
             batch_norm=None,
             conv_dropout=0,
             input_model=None):
    """
    Fully Convolutional Decoder. Copied with permission from github.com/adalca/neurite.

    Parameters:
        ...
        use_skip_connections (bool): if true, turns an Enc-Dec to a U-Net.
            If true, input_tensor and tensors are required.
            It assumes a particular naming of layers. conv_enc...
    """

    # naming
    model_name = name
    if prefix is None:
        prefix = model_name

    # if using skip connections, make sure need to use them.
    if use_skip_connections:
        assert input_model is not None, "is using skip connections, tensors dictionary is required"

    # first layer: input
    input_name = '%s_input' % prefix
    if input_model is None:
        input_tensor = keras.layers.Input(shape=input_shape, name=input_name)
        last_tensor = input_tensor
    else:
        input_tensor = input_model.input
        last_tensor = input_model.output
        input_shape = last_tensor.shape.as_list()[1:]

    # vol size info
    ndims = len(input_shape) - 1
    input_shape = tuple(input_shape)
    if isinstance(pool_size, int):
        if ndims > 1:
            pool_size = (pool_size,) * ndims

    # prepare layers
    convL = getattr(keras.layers, 'Conv%dD' % ndims)
    conv_kwargs = {'padding': padding, 'activation': activation}
    upsample = getattr(keras.layers, 'UpSampling%dD' % ndims)

    # up arm:
    # nb_levels - 1 layers of Deconvolution3D
    #    (approx via up + conv + ReLu) + merge + conv + ReLu + conv + ReLu
    lfidx = 0
    for level in range(nb_levels - 1):
        nb_lvl_feats = np.round(nb_features * feat_mult ** (nb_levels - 2 - level)).astype(int)
        conv_kwargs['dilation_rate'] = dilation_rate_mult ** (nb_levels - 2 - level)

        # upsample matching the max pooling layers size
        name = '%s_up_%d' % (prefix, nb_levels + level)
        last_tensor = upsample(size=pool_size, name=name)(last_tensor)
        up_tensor = last_tensor

        # merge layers combining previous layer
        if use_skip_connections:
            conv_name = '%s_conv_downarm_%d_%d' % (prefix, nb_levels - 2 - level, nb_conv_per_level - 1)
            cat_tensor = input_model.get_layer(conv_name).output
            name = '%s_merge_%d' % (prefix, nb_levels + level)
            last_tensor = keras.layers.concatenate([cat_tensor, last_tensor], axis=ndims + 1, name=name)

        # convolution layers
        for conv in range(nb_conv_per_level):
            if layer_nb_feats is not None:
                nb_lvl_feats = layer_nb_feats[lfidx]
                lfidx += 1

            name = '%s_conv_uparm_%d_%d' % (prefix, nb_levels + level, conv)
            if conv < (nb_conv_per_level - 1) or (not use_residuals):
                last_tensor = convL(nb_lvl_feats, conv_size, **conv_kwargs, name=name)(last_tensor)
            else:
                last_tensor = convL(nb_lvl_feats, conv_size, padding=padding, name=name)(last_tensor)

            if conv_dropout > 0:
                name = '%s_dropout_uparm_%d_%d' % (prefix, level, conv)
                noise_shape = [None, *[1] * ndims, nb_lvl_feats]
                last_tensor = keras.layers.Dropout(conv_dropout, noise_shape=noise_shape)(last_tensor)

        # residual block
        if use_residuals:

            # the "add" layer is the original input
            # However, it may not have the right number of features to be added
            add_layer = up_tensor
            nb_feats_in = add_layer.get_shape()[-1]
            nb_feats_out = last_tensor.get_shape()[-1]
            if nb_feats_in > 1 and nb_feats_out > 1 and (nb_feats_in != nb_feats_out):
                name = '%s_expand_up_merge_%d' % (prefix, level)
                add_layer = convL(nb_lvl_feats, conv_size, **conv_kwargs, name=name)(add_layer)

                if conv_dropout > 0:
                    name = '%s_dropout_up_merge_%d_%d' % (prefix, level, conv)
                    noise_shape = [None, *[1] * ndims, nb_lvl_feats]
                    last_tensor = keras.layers.Dropout(conv_dropout, noise_shape=noise_shape)(last_tensor)

            name = '%s_res_up_merge_%d' % (prefix, level)
            last_tensor = keras.layers.add([last_tensor, add_layer], name=name)

            name = '%s_res_up_merge_act_%d' % (prefix, level)
            last_tensor = keras.layers.Activation(activation, name=name)(last_tensor)

        if batch_norm is not None:
            name = '%s_bn_up_%d' % (prefix, level)
            last_tensor = keras.layers.BatchNormalization(axis=batch_norm, name=name)(last_tensor)

    # Compute likelyhood prediction (no activation yet)
    name = '%s_likelihood' % prefix
    last_tensor = convL(nb_labels, 1, activation=None, name=name)(last_tensor)
    like_tensor = last_tensor

    # output prediction layer
    # we use a softmax to compute P(L_x|I) where x is each location
    if final_pred_activation == 'softmax':
        # print("using final_pred_activation %s for %s" % (final_pred_activation, model_name))
        name = '%s_prediction' % prefix
        softmax_lambda_fcn = lambda x: keras.activations.softmax(x, axis=ndims + 1)
        pred_tensor = keras.layers.Lambda(softmax_lambda_fcn, name=name)(last_tensor)

    # otherwise create a layer that does nothing.
    else:
        name = '%s_prediction' % prefix
        pred_tensor = keras.layers.Activation('linear', name=name)(like_tensor)

    # create the model and retun
    model = keras.Model(inputs=input_tensor, outputs=pred_tensor, name=model_name)
    return model


# ================================================================================================
#                                        Lab2Im Utilities
# 
# TODO: These should be either replaced or reasonably integrated with FS library utilities
# ================================================================================================


# ---------------------------------------------- loading/saving functions ----------------------------------------------


def load_volume(path_volume, im_only=True, squeeze=True, dtype=None, aff_ref=None):
    """
    Load volume file.
    :param path_volume: path of the volume to load. Can either be a nii, nii.gz, mgz, or npz format.
    If npz format, 1) the variable name is assumed to be 'vol_data',
    2) the volume is associated with a identity affine matrix and blank header.
    :param im_only: (optional) if False, the function also returns the affine matrix and header of the volume.
    :param squeeze: (optional) whether to squeeze the volume when loading.
    :param dtype: (optional) if not None, convert the loaded volume to this numpy dtype.
    :param aff_ref: (optional) If not None, the loaded volume is aligned to this affine matrix.
    The returned affine matrix is also given in this new space. Must be a numpy array of dimension 4x4.
    :return: the volume, with corresponding affine matrix and header if im_only is False.
    """
    assert path_volume.endswith(('.nii', '.nii.gz', '.mgz', '.npz')), 'Unknown data file: %s' % path_volume

    if path_volume.endswith(('.nii', '.nii.gz', '.mgz')):
        x = nib.load(path_volume)
        if squeeze:
            volume = np.squeeze(x.get_data())
        else:
            volume = x.get_data()
        aff = x.affine
        header = x.header
    else:  # npz
        volume = np.load(path_volume)['vol_data']
        if squeeze:
            volume = np.squeeze(volume)
        aff = np.eye(4)
        header = nib.Nifti1Header()
    if dtype is not None:
        volume = volume.astype(dtype=dtype)

    # align image to reference affine matrix
    if aff_ref is not None:
        n_dims, _ = get_dims(list(volume.shape), max_channels=3)
        volume, aff = align_volume_to_ref(volume, aff, aff_ref=aff_ref, return_aff=True, n_dims=n_dims)

    if im_only:
        return volume
    else:
        return volume, aff, header


def save_volume(volume, aff, header, path, res=None, dtype=None, n_dims=3):
    """
    Save a volume.
    :param volume: volume to save
    :param aff: affine matrix of the volume to save. If aff is None, the volume is saved with an identity affine matrix.
    aff can also be set to 'FS', in which case the volume is saved with the affine matrix of FreeSurfer outputs.
    :param header: header of the volume to save. If None, the volume is saved with a blank header.
    :param path: path where to save the volume.
    :param res: (optional) update the resolution in the header before saving the volume.
    :param dtype: (optional) numpy dtype for the saved volume.
    :param n_dims: (optional) number of dimensions, to avoid confusion in multi-channel case. Default is None, where
    n_dims is automatically inferred.
    """
    if dtype is not None:
        volume = volume.astype(dtype=dtype)
    if '.npz' in path:
        np.savez_compressed(path, vol_data=volume)
    else:
        if header is None:
            header = nib.Nifti1Header()
        if isinstance(aff, str):
            if aff == 'FS':
                aff = np.array([[-1, 0, 0, 0], [0, 0, 1, 0], [0, -1, 0, 0], [0, 0, 0, 1]])
        elif aff is None:
            aff = np.eye(4)
        nifty = nib.Nifti1Image(volume, aff, header)
        if res is not None:
            if n_dims is None:
                n_dims, _ = get_dims(volume.shape)
            res = reformat_to_list(res, length=n_dims, dtype=None)
            nifty.header.set_zooms(res)
        nib.save(nifty, path)


def get_volume_info(path_volume, return_volume=False, aff_ref=None):
    """
    Gather information about a volume: shape, affine matrix, number of dimensions and channels, header, and resolution.
    :param path_volume: path of the volume to get information form.
    :param return_volume: (optional) whether to return the volume along with the information.
    :param aff_ref: (optional) If not None, the loaded volume is aligned to this affine matrix.
    All info relative to the volume is then given in this new space. Must be a numpy array of dimension 4x4.
    :return: volume (if return_volume is true), and corresponding info.
    """
    # read image
    im, aff, header = load_volume(path_volume, im_only=False)

    # understand if image is multichannel
    im_shape = list(im.shape)
    n_dims, n_channels = get_dims(im_shape, max_channels=3)
    im_shape = im_shape[:n_dims]

    # get labels res
    if '.nii.gz' in path_volume:
        data_res = np.array(header['pixdim'][1:n_dims + 1]).tolist()
    elif '.mgz' in path_volume:
        data_res = np.array(header['delta']).tolist()  # mgz image
    else:
        data_res = [1.0] * n_dims

    # align to given affine matrix
    if aff_ref is not None:
        ras_axes = get_ras_axes(aff, n_dims=n_dims)
        ras_axes_ref = get_ras_axes(aff_ref, n_dims=n_dims)
        im = align_volume_to_ref(im, aff, aff_ref=aff_ref, n_dims=n_dims)
        im_shape = np.array(im_shape)
        data_res = np.array(data_res)
        im_shape[ras_axes_ref] = im_shape[ras_axes]
        data_res[ras_axes_ref] = data_res[ras_axes]
        im_shape = im_shape.tolist()
        data_res = data_res.tolist()

    # return info
    if return_volume:
        return im, im_shape, aff, n_dims, n_channels, header, data_res
    else:
        return im_shape, aff, n_dims, n_channels, header, data_res


def load_array_if_path(var, load_as_numpy=True):
    """If var is a string and load_as_numpy is True, this function loads the array writen at the path indicated by var.
    Otherwise it simply returns var as it is."""
    if (isinstance(var, str)) & load_as_numpy:
        assert os.path.isfile(var), 'No such path: %s' % var
        var = np.load(var)
    return var


def reformat_to_list(var, length=None, load_as_numpy=False, dtype=None):
    """This function takes a variable and reformat it into a list of desired
    length and type (int, float, bool, str).
    If variable is a string, and load_as_numpy is True, it will be loaded as a numpy array.
    If variable is None, this funtion returns None.
    :param var: a str, int, float, list, tuple, or numpy array
    :param length: (optional) if var is a single item, it will be replicated to a list of this length
    :param load_as_numpy: (optional) whether var is the path to a numpy array
    :param dtype: (optional) convert all item to this type. Can be 'int', 'float', 'bool', or 'str'
    :return: reformated list
    """

    # convert to list
    if var is None:
        return None
    var = load_array_if_path(var, load_as_numpy=load_as_numpy)
    if isinstance(var, (int, float)):
        var = [var]
    elif isinstance(var, tuple):
        var = list(var)
    elif isinstance(var, np.ndarray):
        var = np.squeeze(var).tolist()
    elif isinstance(var, str):
        var = [var]
    if isinstance(var, list):
        if length is not None:
            if len(var) == 1:
                var = var * length
            elif len(var) != length:
                raise ValueError('if var is a list/tuple/numpy array, it should be of length 1 or {0}, '
                                 'had {1}'.format(length, var))
    else:
        raise TypeError('var should be an int, float, tuple, list, numpy array, or path to numpy array')

    # convert items type
    if dtype is not None:
        if dtype == 'int':
            var = [int(v) for v in var]
        elif dtype == 'float':
            var = [float(v) for v in var]
        elif dtype == 'bool':
            var = [bool(v) for v in var]
        elif dtype == 'str':
            var = [str(v) for v in var]
        else:
            raise ValueError("dtype should be 'str', 'float', 'int', or 'bool'; had {}".format(dtype))
    return var

# ----------------------------------------------- path-related functions -----------------------------------------------


def list_images_in_folder(path_dir):
    """List all files with extension nii, nii.gz, mgz, or npz whithin a folder."""
    list_images = sorted(glob.glob(os.path.join(path_dir, '*nii.gz')) +
                         glob.glob(os.path.join(path_dir, '*nii')) +
                         glob.glob(os.path.join(path_dir, '*.mgz')) +
                         glob.glob(os.path.join(path_dir, '*.npz')))
    assert len(list_images) > 0, 'no nii, nii.gz, mgz or npz could be found in %s' % path_dir
    return list_images


def list_subfolders(path_dir, whole_path=True, expr=None, cond_type='or'):
    """This function returns a list of subfolders contained in a folder, with possible regexp.
    :param path_dir: path of a folder
    :param whole_path: (optional) whether to return whole path or just the subfolder names.
    :param expr: (optional) regexp for files to list. Can be a str or a list of str.
    :param cond_type: (optional) if exp is a list, specify the logical link between expressions in exp.
    Can be 'or', or 'and'.
    :return: a list of subfolders
    """
    assert isinstance(whole_path, bool), "whole_path should be bool"
    assert cond_type in ['or', 'and'], "cond_type should be either 'or', or 'and'"
    if whole_path:
        subdirs_list = sorted([os.path.join(path_dir, f) for f in os.listdir(path_dir)
                               if os.path.isdir(os.path.join(path_dir, f))])
    else:
        subdirs_list = sorted([f for f in os.listdir(path_dir) if os.path.isdir(os.path.join(path_dir, f))])
    if expr is not None:  # assumed to be either str or list of str
        if isinstance(expr, str):
            expr = [expr]
        elif not isinstance(expr, (list, tuple)):
            raise Exception("if specified, 'expr' should be a string or list of strings.")
        matched_list_subdirs = list()
        for match in expr:
            tmp_matched_list_subdirs = sorted([f for f in subdirs_list if match in os.path.basename(f)])
            if cond_type == 'or':
                subdirs_list = [f for f in subdirs_list if f not in tmp_matched_list_subdirs]
                matched_list_subdirs += tmp_matched_list_subdirs
            elif cond_type == 'and':
                subdirs_list = tmp_matched_list_subdirs
                matched_list_subdirs = tmp_matched_list_subdirs
        subdirs_list = sorted(matched_list_subdirs)
    return subdirs_list


# ---------------------------------------------- shape-related functions -----------------------------------------------


def get_dims(shape, max_channels=3):
    """Get the number of dimensions and channels from the shape of an array.
    The number of dimensions is assumed to be the length of the shape, as long as the shape of the last dimension is
    inferior or equal to max_channels (default 3).
    :param shape: shape of an array. Can be a sequence or a 1d numpy array.
    :param max_channels: maximum possible number of channels.
    :return: the number of dimensions and channels associated with the provided shape.
    example 1: get_dims([150, 150, 150], max_channels=3) = (3, 1)
    example 2: get_dims([150, 150, 150, 3], max_channels=3) = (3, 3)
    example 3: get_dims([150, 150, 150, 5], max_channels=10) = (3, 5)"""
    if shape[-1] <= max_channels:
        n_dims = len(shape) - 1
        n_channels = shape[-1]
    else:
        n_dims = len(shape)
        n_channels = 1
    return n_dims, n_channels


def add_axis(x, axis=0):
    """Add axis to a numpy array. The new axis can be added to the first dimension (axis=0), to the last dimension
    (axis=-1), or to both (axis=-2)."""
    if axis == 0:
        return x[np.newaxis, ...]
    elif axis == -1:
        return x[..., np.newaxis]
    elif axis == -2:
        return x[np.newaxis, ..., np.newaxis]
    else:
        raise Exception('axis should be 0 (first), -1 (last), or -2 (first and last)')


# --------------------------------------------------- miscellaneous ----------------------------------------------------


def print_loop_info(idx, n_iterations, spacing):
    """Print loop iteration number.
    :param idx: iteration number
    :param n_iterations: total number iterations.
    :param spacing: frequency at which to print loop advancement.
    """
    if idx == 0:
        print('processing {}/{}'.format(1, n_iterations))
    elif idx % spacing == spacing - 1:
        print('processing {}/{}'.format(idx + 1, n_iterations))


def find_closest_number_divisible_by_m(n, m, smaller_ans=True):
    """Return the closest integer to n that is divisible by m.
    If smaller_ans is True, only values lower than n are considered."""
    # quotient
    q = int(n / m)
    # 1st possible closest number
    n1 = m * q
    # 2nd possible closest number
    if (n * m) > 0:
        n2 = (m * (q + 1))
    else:
        n2 = (m * (q - 1))
    # find closest solution
    if (abs(n - n1) < abs(n - n2)) | smaller_ans:
        return n1
    else:
        return n2


# ---------------------------------------------------- edit volume -----------------------------------------------------


def crop_volume_with_idx(volume, crop_idx, aff=None):
    """Crop a volume with given indices.
    :param volume: a 2d or 3d numpy array
    :param crop_idx: croppping indices, in the order [lower_bound_dim_1, ..., upper_bound_dim_1, ...].
    Can be a list or a 1d numpy array.
    :param aff: (optional) if specified, this function returns an updated affine matrix of the volume after cropping.
    :return: the cropped volume, and the updated affine matrix if aff is not None.
    """

    # crop image
    n_dims = int(crop_idx.shape[0] / 2)
    if n_dims == 2:
        volume = volume[crop_idx[0]:crop_idx[2], crop_idx[1]:crop_idx[3], ...]
    elif n_dims == 3:
        volume = volume[crop_idx[0]:crop_idx[3], crop_idx[1]:crop_idx[4], crop_idx[2]:crop_idx[5], ...]
    else:
        raise Exception('cannot crop volumes with more than 3 dimensions')

    if aff is not None:
        aff[0:3, -1] = aff[0:3, -1] + aff[:3, :3] @ crop_idx[:3]
        return volume, aff
    else:
        return volume


def get_ras_axes(aff, n_dims=3):
    """This function finds the RAS axes corresponding to each dimension of a volume, based on its affine matrix.
    :param aff: affine matrix Can be a 2d numpy array of size n_dims*n_dims, n_dims+1*n_dims+1, or n_dims*n_dims+1.
    :param n_dims: number of dimensions (excluding channels) of the volume corresponding to the provided affine matrix.
    :return: two numpy 1d arrays of lengtn n_dims, one with the axes corresponding to RAS orientations,
    and one with their corresponding direction.
    """
    aff_inverted = np.linalg.inv(aff)
    img_ras_axes = np.argmax(np.absolute(aff_inverted[0:n_dims, 0:n_dims]), axis=0)
    return img_ras_axes


def align_volume_to_ref(volume, aff, aff_ref=None, return_aff=False, n_dims=None):
    """This function aligns a volume to a reference orientation (axis and direction) specified by an affine matrix.
    :param volume: a numpy array
    :param aff: affine matrix of the floating volume
    :param aff_ref: (optional) affine matrix of the target orientation. Default is identity matrix.
    :param return_aff: (optional) whether to return the affine matrix of the aligned volume
    :param n_dims: number of dimensions (excluding channels) of the volume corresponding to the provided affine matrix.
    :return: aligned volume, with corresponding affine matrix if return_aff is True.
    """

    # work on copy
    aff_flo = aff.copy()

    # default value for aff_ref
    if aff_ref is None:
        aff_ref = np.eye(4)

    # extract ras axes
    if n_dims is None:
        n_dims, _ = get_dims(volume.shape)
    ras_axes_ref = get_ras_axes(aff_ref, n_dims=n_dims)
    ras_axes_flo = get_ras_axes(aff_flo, n_dims=n_dims)

    # align axes
    aff_flo[:, ras_axes_ref] = aff_flo[:, ras_axes_flo]
    for i in range(n_dims):
        if ras_axes_flo[i] != ras_axes_ref[i]:
            volume = np.swapaxes(volume, ras_axes_flo[i], ras_axes_ref[i])
            swapped_axis_idx = np.where(ras_axes_flo == ras_axes_ref[i])
            ras_axes_flo[swapped_axis_idx], ras_axes_flo[i] = ras_axes_flo[i], ras_axes_flo[swapped_axis_idx]

    # align directions
    dot_products = np.sum(aff_flo[:3, :3] * aff_ref[:3, :3], axis=0)
    for i in range(n_dims):
        if dot_products[i] < 0:
            volume = np.flip(volume, axis=i)
            aff_flo[:, i] = - aff_flo[:, i]
            aff_flo[:3, 3] = aff_flo[:3, 3] - aff_flo[:3, i] * (volume.shape[i] - 1)

    if return_aff:
        return volume, aff_flo
    else:
        return volume


# execute script
if __name__ == '__main__':
    main()
